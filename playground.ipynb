{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38d50e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helenz/Desktop/mlx-summariser/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"SUBREDDIT: r/relationships\\nTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\\nPOST: Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\\nTL;DR: \", 'label': \"I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tldr_dataset = load_dataset(\"CarperAI/openai_summarize_tldr\")\n",
    "print(tldr_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'label'],\n",
      "        num_rows: 116722\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'label'],\n",
      "        num_rows: 6553\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['prompt', 'label'],\n",
      "        num_rows: 6447\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tldr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60a2a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92534/92534 [00:00<00:00, 643813.23 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83629/83629 [00:00<00:00, 596744.88 examples/s]\n",
      "Generating valid1 split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33082/33082 [00:00<00:00, 567553.16 examples/s]\n",
      "Generating valid2 split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50715/50715 [00:00<00:00, 593207.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'SUBREDDIT: r/relationships\\nTITLE: To admit or not to admit snooping...\\nPOST: I [25M] have snooped in the past and copped up to it to my gf [25F] of 6 years.  We talked it through.  It had been a year or two since the last time.  That\\'s an issue I\\'m working on.\\n\\nNow she has a new close male work friend.  I won\\'t go into details, but she hides things from me with him and does other things to make me a bit suspicious.  So...I snooped again, and this time, all texts from her new friend have been deleted and I saw a google search for \"how to get over a guy\" near some searches of his name and views of his Facebook profile.\\n\\nI asked her about this guy, not mentioning the snooping, and she denied any feelings, we talked for a long time about our relationship and she insisted that she only loves me and I mean the world to her, and that she really wants to work towards getting this relationship back out of the rut we\\'ve been in (we both work all the time and barely see each other).\\n\\nI think if I cop to the snooping, we might have a more honest conversation about what\\'s actually going on (if something is) and why she\\'s having these feelings so we can either work through it together (my preference) or move on.  But obviously, it will open the pandora\\'s box of the snooping.\\n\\nThink it\\'s worth it to admit to the snooping to hopefully get to the bottom of this?', 'chosen': 'TL;DR:  Snooped, found something, should I admit what I found so we can have a more honest conversation about it with less denial on her part?', 'rejected': \"TL;DR:  I snooped, we talked about it, she wants to work it out, I'm not sure.  Is the snooping worth it?\"}\n"
     ]
    }
   ],
   "source": [
    "comparison_dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\")\n",
    "print(comparison_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e6f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 92534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 83629\n",
      "    })\n",
      "    valid1: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 33082\n",
      "    })\n",
      "    valid2: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 50715\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(comparison_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length=550):\n",
    "        dataset = load_dataset(train_path, split=split)\n",
    "        self.examples = [sample[\"prompt\"] + sample[\"label\"] for sample in dataset]\n",
    "        self.examples = self.examples[:2000] if \"valid\" in split else self.examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.examples[idx], truncation=True, max_length=self.max_length, padding=\"max_length\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(enc[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(enc[\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(enc[\"input_ids\"]),  # teacher forcing\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5dce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\n",
      "POST: Not sure if this belongs here but it's worth a try. \n",
      "\n",
      "Backstory:\n",
      "When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \n",
      "\n",
      "Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \n",
      "\n",
      "His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \n",
      "\n",
      "So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\n",
      "TL;DR: I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\n"
     ]
    }
   ],
   "source": [
    "tldr_dataset_test = tldr_dataset[\"train\"].select(range(100))\n",
    "examples = [sample[\"prompt\"] + sample[\"label\"] for sample in tldr_dataset_test]\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb27879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "qwen_name = \"Qwen/Qwen3-0.6B-Base\"\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26105ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "qwen_tokenizer.pad_token = qwen_tokenizer.eos_token\n",
    "print(qwen_tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d33f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 358, 320, 69, 14, 17, 17, 8, 614, 311, 7071, 700, 421, 358, 1366, 311, 2058, 1414, 1493, 7571, 476, 537, 323, 1035, 12213, 311, 5112, 67092, 198, 2946, 25, 2806, 2704, 421, 419, 17180, 1588, 714, 432, 594, 5802, 264, 1430, 13, 4710, 3707, 26485, 510, 4498, 358, 320, 69, 14, 17, 17, 8, 3937, 1526, 847, 1156, 1931, 84498, 220, 17, 1635, 4134, 1576, 566, 4362, 3550, 1283, 264, 1042, 315, 4924, 926, 437, 220, 432, 88489, 752, 803, 1091, 358, 3381, 13, 1084, 572, 264, 27102, 882, 304, 847, 2272, 4152, 311, 5382, 448, 847, 6554, 323, 5499, 3432, 279, 6012, 311, 3931, 1059, 700, 315, 847, 2272, 13, 358, 646, 16698, 1576, 315, 432, 572, 458, 14269, 35750, 323, 419, 7412, 572, 15175, 323, 3207, 944, 1414, 1246, 311, 3484, 448, 752, 13, 1205, 9482, 553, 1435, 30426, 369, 264, 2254, 476, 773, 1283, 2087, 311, 264, 18780, 448, 847, 4780, 13, 3197, 358, 1744, 1182, 358, 6426, 566, 1101, 9482, 13, 2055, 1283, 566, 9482, 432, 3694, 847, 18210, 358, 16256, 714, 847, 4780, 8910, 752, 1526, 432, 323, 358, 2684, 9279, 315, 4297, 504, 1435, 3156, 448, 14376, 3645, 13, 4710, 7039, 25, 11445, 1012, 4558, 220, 18, 1635, 1431, 323, 358, 3003, 17019, 2664, 1283, 82681, 323, 23034, 7147, 38200, 1783, 13, 3017, 6554, 702, 1012, 700, 315, 847, 2272, 2474, 1221, 773, 1052, 594, 1012, 56238, 315, 5098, 13, 20690, 16245, 1283, 6832, 1045, 18366, 1052, 1012, 803, 20017, 911, 429, 882, 315, 847, 2272, 714, 979, 358, 1490, 1435, 476, 264, 6802, 4297, 4041, 1182, 13, 576, 21261, 323, 18923, 4446, 752, 1182, 1495, 13, 4710, 15986, 4780, 320, 21028, 7571, 8, 525, 389, 847, 22943, 1576, 582, 633, 3156, 1632, 892, 374, 2588, 311, 1477, 323, 358, 1414, 807, 3278, 2677, 614, 806, 1182, 13, 1988, 9120, 1435, 304, 264, 6802, 476, 7404, 311, 1435, 518, 264, 21277, 3432, 264, 10435, 374, 11045, 13, 356, 27509, 16877, 315, 847, 1482, 25838, 374, 2494, 358, 1366, 311, 5648, 13, 4710, 4416, 358, 3003, 1012, 7274, 429, 358, 614, 311, 3931, 3645, 448, 1493, 7571, 1576, 432, 594, 882, 311, 3271, 389, 1576, 432, 594, 38245, 13, 1084, 594, 1850, 311, 5648, 1435, 438, 1632, 13, 1988, 686, 807, 387, 26132, 291, 30, 4841, 807, 4193, 432, 30, 2160, 1052, 2087, 311, 387, 28759, 2090, 30, 358, 2776, 537, 2704, 421, 432, 594, 279, 1290, 311, 653, 323, 1410, 990, 1045, 4889, 17979, 624, 13470, 26, 7687, 25, 358, 2058, 614, 3645, 448, 458, 2310, 505, 594, 4780, 714, 646, 944, 2498, 311, 1490, 476, 3061, 311, 1435, 13, 5301, 4780, 525, 2167, 6419, 1154, 704, 1246, 653, 358, 3291, 1105, 358, 10767, 1366, 311, 9474, 5039, 1105, 389, 5573, 1576, 315, 1435, 30, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "enc = qwen_tokenizer(\n",
    "    examples[index], truncation=True, max_length=550, padding=\"max_length\"\n",
    ")\n",
    "print(enc[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b82cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(enc[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857db3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(qwen_name, use_cache=False)\n",
    "# Resize token embeddings / adjust token count\n",
    "model.resize_token_embeddings(len(qwen_tokenizer)) \n",
    "model.config.pad_token_id = qwen_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c57f492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151669, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151669, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ecf345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”  Tokenizer vocab size: 151669\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ”  Tokenizer vocab size: {len(qwen_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce801c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-summariser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
